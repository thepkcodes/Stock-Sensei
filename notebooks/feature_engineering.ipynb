{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Feature Engineering for Stock Market AI Agent\n",
    "\n",
    "This notebook performs comprehensive feature engineering on our stock market datasets including:\n",
    "\n",
    "1. **Technical Indicators**: Moving averages, RSI, MACD, Bollinger Bands, etc.\n",
    "2. **Price Features**: Returns, volatility, price ratios\n",
    "3. **Volume Features**: Volume indicators and price-volume relationships\n",
    "4. **Sentiment Features**: Sentiment moving averages, momentum, extremes\n",
    "5. **Macro Features**: Economic indicator derivatives and regime indicators\n",
    "6. **Lagged Features**: Historical values for time series modeling\n",
    "7. **Target Variables**: Next day/week price predictions\n",
    "\n",
    "**Goal**: Create a comprehensive feature set for machine learning models to predict stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from feature_engineering import (\n",
    "    StockFeatureEngineer, \n",
    "    SentimentFeatureEngineer, \n",
    "    MacroFeatureEngineer, \n",
    "    DataMerger\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw datasets\n",
    "print(\"Loading datasets...\")\n",
    "stock_data = pd.read_csv('../data/stock_prices.csv')\n",
    "sentiment_data = pd.read_csv('../data/news_sentiment.csv')\n",
    "macro_data = pd.read_csv('../data/macro_indicators.csv')\n",
    "\n",
    "# Convert dates\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "sentiment_data['Date'] = pd.to_datetime(sentiment_data['Date'])\n",
    "macro_data['Date'] = pd.to_datetime(macro_data['Date'])\n",
    "\n",
    "print(f\"✅ Stock data: {stock_data.shape}\")\n",
    "print(f\"✅ Sentiment data: {sentiment_data.shape}\")\n",
    "print(f\"✅ Macro data: {macro_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Initialize Feature Engineers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineering classes\n",
    "stock_fe = StockFeatureEngineer()\n",
    "sentiment_fe = SentimentFeatureEngineer()\n",
    "macro_fe = MacroFeatureEngineer()\n",
    "merger = DataMerger()\n",
    "\n",
    "print(\"✅ Feature engineers initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Sentiment Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Processing sentiment data...\")\n",
    "\n",
    "# Forward fill sentiment data per ticker (as specified in requirements)\n",
    "sentiment_processed = sentiment_fe.forward_fill_sentiment(sentiment_data)\n",
    "print(f\"Forward fill completed\")\n",
    "\n",
    "# Create sentiment features\n",
    "sentiment_processed = sentiment_fe.create_sentiment_features(sentiment_processed)\n",
    "print(f\"✅ Sentiment features created: {len(sentiment_fe.features_created)}\")\n",
    "print(f\"Features: {sentiment_fe.features_created[:10]}...\")  # Show first 10\n",
    "\n",
    "print(f\"Final sentiment data shape: {sentiment_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Macroeconomic Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Processing macroeconomic data...\")\n",
    "\n",
    "# Create macro features\n",
    "macro_processed = macro_fe.create_macro_features(macro_data)\n",
    "print(f\"✅ Macro features created: {len(macro_fe.features_created)}\")\n",
    "print(f\"Features: {macro_fe.features_created[:10]}...\")  # Show first 10\n",
    "\n",
    "print(f\"Final macro data shape: {macro_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Stock Technical Feature Engineering\n",
    "\n",
    "We'll process each stock ticker individually to ensure proper calculation of technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Processing stock data...\")\n",
    "print(f\"Total tickers to process: {stock_data['Ticker'].nunique()}\")\n",
    "\n",
    "processed_stock_data = []\n",
    "tickers = stock_data['Ticker'].unique()\n",
    "\n",
    "# Process first few tickers for demonstration (you can process all by removing [:5])\n",
    "sample_tickers = tickers[:5]  # Remove [:5] to process all tickers\n",
    "\n",
    "for i, ticker in enumerate(sample_tickers):\n",
    "    print(f\"Processing {ticker} ({i+1}/{len(sample_tickers)})...\")\n",
    "    \n",
    "    # Get ticker data\n",
    "    ticker_data = stock_data[stock_data['Ticker'] == ticker].copy()\n",
    "    ticker_data = ticker_data.sort_values('Date')\n",
    "    \n",
    "    # Create all stock features\n",
    "    ticker_data = stock_fe.create_price_features(ticker_data)\n",
    "    ticker_data = stock_fe.create_moving_averages(ticker_data)\n",
    "    ticker_data = stock_fe.create_volatility_features(ticker_data)\n",
    "    ticker_data = stock_fe.create_momentum_features(ticker_data)\n",
    "    ticker_data = stock_fe.create_volume_features(ticker_data)\n",
    "    ticker_data = stock_fe.create_lagged_features(ticker_data)\n",
    "    ticker_data = stock_fe.create_target_variables(ticker_data)\n",
    "    \n",
    "    processed_stock_data.append(ticker_data)\n",
    "\n",
    "# Combine all processed stock data\n",
    "processed_stock_data = pd.concat(processed_stock_data, ignore_index=True)\n",
    "\n",
    "print(f\"✅ Stock features created: {len(stock_fe.features_created)}\")\n",
    "print(f\"Final processed stock data shape: {processed_stock_data.shape}\")\n",
    "print(f\"Sample features: {stock_fe.features_created[:15]}...\")  # Show first 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Data Merging\n",
    "\n",
    "Now we'll merge all the processed datasets into one comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Merging all datasets...\")\n",
    "\n",
    "# Merge all datasets\n",
    "final_data = merger.merge_all_data(processed_stock_data, sentiment_processed, macro_processed)\n",
    "print(\"✅ Initial merge completed\")\n",
    "\n",
    "# Clean merged data\n",
    "final_data = merger.clean_merged_data(final_data)\n",
    "print(\"✅ Data cleaning completed\")\n",
    "\n",
    "print(f\"\\n📊 Final Dataset Summary:\")\n",
    "print(f\"Shape: {final_data.shape}\")\n",
    "print(f\"Date range: {final_data['Date'].min().date()} to {final_data['Date'].max().date()}\")\n",
    "print(f\"Tickers: {sorted(final_data['Ticker'].unique())}\")\n",
    "print(f\"Total features: {len(final_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature categories\n",
    "all_features = final_data.columns.tolist()\n",
    "\n",
    "# Categorize features\n",
    "price_features = [f for f in all_features if any(x in f for x in ['price', 'return', 'close', 'open', 'high', 'low'])]\n",
    "volume_features = [f for f in all_features if 'volume' in f or 'obv' in f or 'vpt' in f]\n",
    "technical_features = [f for f in all_features if any(x in f for x in ['sma', 'ema', 'rsi', 'macd', 'bb_', 'atr', 'stoch'])]\n",
    "sentiment_features = [f for f in all_features if 'sentiment' in f or 'news' in f]\n",
    "macro_features = [f for f in all_features if any(x in f for x in ['GDP', 'unemployment', 'inflation', 'federal', 'consumer', 'vix'])]\n",
    "target_features = [f for f in all_features if 'target' in f]\n",
    "\n",
    "print(\"📊 Feature Categories:\")\n",
    "print(f\"Price features: {len(price_features)}\")\n",
    "print(f\"Volume features: {len(volume_features)}\")\n",
    "print(f\"Technical indicators: {len(technical_features)}\")\n",
    "print(f\"Sentiment features: {len(sentiment_features)}\")\n",
    "print(f\"Macro features: {len(macro_features)}\")\n",
    "print(f\"Target variables: {len(target_features)}\")\n",
    "print(f\"Other features: {len(all_features) - len(price_features) - len(volume_features) - len(technical_features) - len(sentiment_features) - len(macro_features) - len(target_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some key features for AAPL\n",
    "if 'AAPL' in final_data['Ticker'].values:\n",
    "    aapl_data = final_data[final_data['Ticker'] == 'AAPL'].copy()\n",
    "    aapl_data = aapl_data.sort_values('Date')\n",
    "    \n",
    "    # Plot some key features\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Price and moving averages\n",
    "    axes[0].plot(aapl_data['Date'], aapl_data['close'], label='Close Price', alpha=0.8)\n",
    "    if 'sma_20' in aapl_data.columns:\n",
    "        axes[0].plot(aapl_data['Date'], aapl_data['sma_20'], label='SMA 20', alpha=0.7)\n",
    "    if 'sma_50' in aapl_data.columns:\n",
    "        axes[0].plot(aapl_data['Date'], aapl_data['sma_50'], label='SMA 50', alpha=0.7)\n",
    "    axes[0].set_title('AAPL: Price and Moving Averages')\n",
    "    axes[0].legend()\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # RSI\n",
    "    if 'rsi_14' in aapl_data.columns:\n",
    "        axes[1].plot(aapl_data['Date'], aapl_data['rsi_14'])\n",
    "        axes[1].axhline(70, color='red', linestyle='--', alpha=0.7, label='Overbought')\n",
    "        axes[1].axhline(30, color='green', linestyle='--', alpha=0.7, label='Oversold')\n",
    "        axes[1].set_title('AAPL: RSI (14)')\n",
    "        axes[1].legend()\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Volatility\n",
    "    if 'volatility_20d' in aapl_data.columns:\n",
    "        axes[2].plot(aapl_data['Date'], aapl_data['volatility_20d'])\n",
    "        axes[2].set_title('AAPL: 20-Day Volatility')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sentiment\n",
    "    if 'sentiment_score' in aapl_data.columns:\n",
    "        axes[3].plot(aapl_data['Date'], aapl_data['sentiment_score'], alpha=0.6)\n",
    "        if 'sentiment_sma_7' in aapl_data.columns:\n",
    "            axes[3].plot(aapl_data['Date'], aapl_data['sentiment_sma_7'], label='7-day SMA')\n",
    "        axes[3].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[3].set_title('AAPL: Sentiment Score')\n",
    "        axes[3].legend()\n",
    "        axes[3].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Volume ratio\n",
    "    if 'volume_ratio_20' in aapl_data.columns:\n",
    "        axes[4].plot(aapl_data['Date'], aapl_data['volume_ratio_20'])\n",
    "        axes[4].axhline(1, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[4].set_title('AAPL: Volume Ratio (20-day)')\n",
    "        axes[4].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Returns distribution\n",
    "    if 'return_1d' in aapl_data.columns:\n",
    "        axes[5].hist(aapl_data['return_1d'].dropna(), bins=50, alpha=0.7)\n",
    "        axes[5].set_title('AAPL: Daily Returns Distribution')\n",
    "        axes[5].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n    print(\"AAPL data not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 8. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key features for correlation analysis\n",
    "key_features = [\n",
    "    'close', 'volume', 'return_1d', 'volatility_20d',\n",
    "    'rsi_14', 'sma_20_ratio', 'bb_position',\n",
    "    'sentiment_score', 'sentiment_sma_7',\n",
    "    'unemployment_rate', 'inflation_rate', 'vix'\n",
    "]\n",
    "\n",
    "# Filter to features that exist in our dataset\n",
    "available_features = [f for f in key_features if f in final_data.columns]\n",
    "\n",
    "if len(available_features) > 2:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_data = final_data[available_features].select_dtypes(include=[np.number])\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                square=True, \n",
    "                fmt='.2f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Correlation Matrix of Key Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🔍 High correlations (> 0.7):\")\n",
    "    high_corr = correlation_matrix.abs() > 0.7\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if high_corr.iloc[i, j]:\n",
    "                print(f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")\nelse:\n    print(\"Not enough features available for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 9. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "missing_values = final_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(final_data)) * 100\n",
    "\n",
    "# Create missing values summary\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Filter to features with missing values\n",
    "missing_features = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "if not missing_features.empty:\n",
    "    print(\"🔍 Features with Missing Values:\")\n",
    "    print(missing_features.head(20))\n",
    "    \n",
    "    # Plot missing values\n",
    "    if len(missing_features) > 0:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_missing = missing_features.head(20)\n",
    "        plt.barh(range(len(top_missing)), top_missing['Missing_Percentage'])\n",
    "        plt.yticks(range(len(top_missing)), top_missing.index)\n",
    "        plt.xlabel('Missing Percentage (%)')\n",
    "        plt.title('Top 20 Features with Missing Values')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\nelse:\n    print(\"✅ No missing values found in the dataset!\")\n\nprint(f\"\\n📊 Overall Missing Values: {final_data.isnull().sum().sum()} out of {final_data.size} total values\")\nprint(f\"Missing percentage: {(final_data.isnull().sum().sum() / final_data.size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset\n",
    "print(\"💾 Saving processed data...\")\n",
    "final_data.to_csv('../data/processed_features.csv', index=False)\n",
    "print(\"✅ Processed data saved to '../data/processed_features.csv'\")\n",
    "\n",
    "# Create and save feature summary\n",
    "feature_summary = {\n",
    "    'total_features': len(final_data.columns),\n",
    "    'total_records': len(final_data),\n",
    "    'date_range': {\n",
    "        'start': final_data['Date'].min().strftime('%Y-%m-%d'),\n",
    "        'end': final_data['Date'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'tickers': sorted(final_data['Ticker'].unique().tolist()),\n",
    "    'feature_categories': {\n",
    "        'price_features': len(price_features),\n",
    "        'volume_features': len(volume_features),\n",
    "        'technical_features': len(technical_features),\n",
    "        'sentiment_features': len(sentiment_features),\n",
    "        'macro_features': len(macro_features),\n",
    "        'target_features': len(target_features)\n",
    "    },\n",
    "    'stock_features_created': stock_fe.features_created,\n",
    "    'sentiment_features_created': sentiment_fe.features_created,\n",
    "    'macro_features_created': macro_fe.features_created,\n",
    "    'all_features': final_data.columns.tolist()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/feature_summary.json', 'w') as f:\n",
    "    json.dump(feature_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ Feature summary saved to '../data/feature_summary.json'\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n🎉 Feature Engineering Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Final Dataset:\")\n",
    "print(f\"   Shape: {final_data.shape}\")\n",
    "print(f\"   Total Features: {len(final_data.columns)}\")\n",
    "print(f\"   Date Range: {final_data['Date'].min().date()} to {final_data['Date'].max().date()}\")\n",
    "print(f\"   Tickers: {len(final_data['Ticker'].unique())}\")\n",
    "print(f\"   Records per ticker (avg): {len(final_data) // len(final_data['Ticker'].unique()):.0f}\")\n",
    "\n",
    "print(f\"\\n🔧 Features Created:\")\n",
    "print(f\"   Stock/Technical: {len(stock_fe.features_created)}\")\n",
    "print(f\"   Sentiment: {len(sentiment_fe.features_created)}\")\n",
    "print(f\"   Macro: {len(macro_fe.features_created)}\")\n",
    "\n",
    "print(f\"\\n📈 Ready for Model Training!\")\n",
    "print(f\"   Target variables available: {target_features}\")\n",
    "print(f\"   Next step: Build ML models using this feature-rich dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
